# Configuration for the single-task SegFormer baseline experiment
# using the MiT-B2 backbone.

data:
  batch_size: 4
  num_workers: 4
  pin_memory: true
  patch_size: 512

model:
  # For the baseline, we use the Hugging Face model directly.
  # The encoder name is the primary identifier.
  encoder_name: "nvidia/mit-b2"
  num_classes: 40 # 39 Coralscapes classes + 1 unlabeled

optimizer:
  learning_rate: 6.0e-5
  weight_decay: 0.01
  adam_betas: [0.9, 0.999]

loss:
  # Parameters for the CoralLoss (single-task version)
  primary_loss_type: 'focal'
  hybrid_alpha: 0.5
  focal_gamma: 2.0

training:
  device: "cuda"
  seed: 42
  num_epochs: 50
  gradient_accumulation_steps: 4
  warmup_steps_ratio: 0.1
  output_dir: "experiments/baseline_segformer_run"
  best_model_name: "best_baseline_model.pth"
  debug: false