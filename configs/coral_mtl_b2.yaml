# Configuration for the main Coral-MTL experiment
# using the MiT-B2 backbone.

data:
  batch_size: 4
  num_workers: 4
  pin_memory: true
  patch_size: 512

model:
  encoder_name: "nvidia/mit-b2"
  decoder_channel: 256
  attention_dim: 128

optimizer:
  learning_rate: 6.0e-5
  weight_decay: 0.01
  adam_betas: [0.9, 0.999]

loss:
  w_consistency: 0.1
  hybrid_alpha: 0.5
  focal_gamma: 2.0

training:
  device: "cuda"
  seed: 42
  num_epochs: 100
  gradient_accumulation_steps: 4
  warmup_steps_ratio: 0.1 # 10% of total steps
  output_dir: "experiments/coral_mtl_b2_run"
  best_model_name: "best_model.pth"
  debug: false