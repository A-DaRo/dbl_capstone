# =============================================================================
# CORRECTED AND EXTENDED BASELINE CONFIGURATION FILE
# =============================================================================
# Configuration file for training a SegFormer baseline (non-MTL) model on 
# the Coralscapes dataset. This configuration has been corrected to remove
# duplications and extended with all necessary sections for the ExperimentFactory.

# --------------------------------------------------------------------------
# Configuration for the Model Architecture
# --------------------------------------------------------------------------
model:
  # This flag tells the factory to build the BaselineSegformer.
  type: "SegFormerBaseline"
  params:
    # Name of the SegFormer backbone.
    backbone: "nvidia/mit-b2"
    # Unified channel dimension for the standard MLP decoder.
    decoder_channel: 256
    # For a simple baseline, the number of output classes is specified directly.
    num_classes: 40

# --------------------------------------------------------------------------
# Configuration for Datasets and DataLoaders
# --------------------------------------------------------------------------
data:
  # The factory will first check if this is a valid local path. If not, it assumes
  # it's a Hugging Face Hub dataset ID.
  dataset_name: "EPFL-ECEO/coralscapes"

  # Path to the pre-processed PDS-sampled training patches.
  # This is prioritized for the 'train' split, as specified in dataset.py.
  pds_train_path: "./data/processed/pds_patches/"
  
  # Path to the raw, full-image dataset. Used for 'val' and 'test' splits,
  # and as a fallback for 'train' if pds_train_path is not found.
  data_root_path: "./data/raw/coralscapes_raw/"

  # The single source of truth for all class and task information.
  # Required for MTL models and for non-MTL models being evaluated on MTL tasks.
  task_definitions_path: "configs/task_definitions.yaml"
  
  # Parameters for the final DataLoader object.
  batch_size: 4
  num_workers: 4
  
  # A fundamental parameter shared by datasets and augmentations.
  patch_size: 512

# --------------------------------------------------------------------------
# Configuration for the Augmentation Pipeline (for the 'train' split)
# --------------------------------------------------------------------------
augmentations:
  # The scale range for the RandomResizedCrop transform.
  crop_scale: [0.5, 1.0]
  # The degree range for random rotations.
  rotation_degrees: 15
  # Parameters for the ColorJitter transform.
  jitter_params:
    brightness: 0.2
    contrast: 0.2
    saturation: 0.2
    hue: 0.1

# --------------------------------------------------------------------------
# Configuration for the Loss Function
# --------------------------------------------------------------------------
loss:
  # This type corresponds to the CoralLoss class for baseline models.
  type: "HybridLoss"
  params:
    # The main pixel-wise classification loss. Can be 'focal' or 'cross_entropy'.
    primary_loss_type: "focal"
    hybrid_alpha: 0.5
    focal_gamma: 2.0
    dice_smooth: 1.0
    ignore_index: 0

# --------------------------------------------------------------------------
# Configuration for the Optimizer and LR Scheduler
# --------------------------------------------------------------------------
optimizer:
  # This type is mapped to the `create_optimizer_and_scheduler` function.
  type: "AdamWPolyDecay"
  params:
    lr: 6.0e-5
    weight_decay: 0.01
    adam_betas: [0.9, 0.999]
    # The percentage of total training steps to use for the linear warmup phase.
    warmup_ratio: 0.1
    # The power of the polynomial for the learning rate decay.
    power: 1.0

# --------------------------------------------------------------------------
# Configuration for Metrics Calculation
# --------------------------------------------------------------------------
metrics:
  # This list is crucial. It tells both metrics calculators which tasks should
  # be used to compute the Boundary IoU (BIoU) and the final H-Mean score.
  # The names must match the task names in task_definitions.yaml.
  primary_tasks: ["genus", "health"]
  
  # The thickness of the band (in pixels) used for BIoU calculation.
  boundary_thickness: 2
  
  # The class index to be ignored during metric calculation.
  # This should be consistent with the ignore_index in the loss function.
  ignore_index: 0


# --------------------------------------------------------------------------
# Configuration for the Training Loop
# --------------------------------------------------------------------------
trainer:
  # Number of training epochs
  epochs: 100
  
  # Device configuration - 'auto' will use CUDA if available, else CPU
  device: "auto"
  
  # Output directory for checkpoints, logs, and results
  output_dir: "experiments/baseline_segformer"
  
  # Metric used for model selection (saving best checkpoint)
  # Should be one of the metrics computed by the metrics calculator
  model_selection_metric: "h_mean"
  
  # Whether to maximize (True) or minimize (False) the model selection metric
  metric_mode: "max"
  
  # Frequency of validation evaluation (every N epochs)
  val_frequency: 1
  
  # Frequency of checkpoint saving (every N epochs)
  checkpoint_frequency: 10
  
  # Whether to save only the best model or all checkpoints
  save_best_only: true
  
  # Early stopping patience (number of epochs without improvement)
  early_stopping_patience: 15
  
  # Minimum improvement to consider as progress for early stopping
  min_delta: 1e-4
  
  # Parameters for sliding window inference during validation
  inference_stride: 256
  inference_batch_size: 16
  
  # Whether to use mixed precision training (fp16)
  use_mixed_precision: true
  
  # Gradient clipping maximum norm
  max_grad_norm: 1.0
  
  # Frequency of logging training metrics (every N steps)
  log_frequency: 100

# --------------------------------------------------------------------------
# Configuration for Model Evaluation
# --------------------------------------------------------------------------
evaluator:
  # Path to the model checkpoint for evaluation
  # If not provided, will auto-detect 'best_model.pth' in trainer output_dir
  checkpoint_path: null
  
  # Output directory for evaluation results and visualizations
  # If not provided, will use a subdirectory of trainer output_dir
  output_dir: null
  
  # Number of samples to visualize in qualitative results
  num_visualizations: 8

# --------------------------------------------------------------------------
# Configuration for Visualization and Plotting
# --------------------------------------------------------------------------
visualizer:
  # Matplotlib style for plots
  style: "seaborn-v0_8-whitegrid"
  
  # DPI for saved figures
  dpi: 300
  
  # Figure size for plots (width, height in inches)
  figure_size: [12, 8]
  
  # Color palette for plots
  color_palette: "viridis"
  
  # Whether to show plots interactively (in addition to saving)
  show_plots: false
  
  # File format for saved plots
  plot_format: "png"